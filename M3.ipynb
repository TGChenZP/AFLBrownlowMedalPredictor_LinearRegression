{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3. LR-RS-BT-0.25-1 ##\n",
    "\n",
    "Trains model 3 based on:\n",
    "\n",
    "- Regular Linear Regression\n",
    "- RankStandardised Data \n",
    "- Both Teams data\n",
    "- FS_Val 0.25\n",
    "- FS_Rule 1\n",
    "\n",
    "**Author: `Lang (Ron) Chen` 2021.12-2022.1**\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from BrownlowPredictorTools.predict import predict\n",
    "from BrownlowPredictorTools.test import test\n",
    "from BrownlowPredictorTools.return_tp import return_tp\n",
    "from BrownlowPredictorTools.wholeseason import wholeseason\n",
    "from BrownlowPredictorTools.feature_selection2 import feature_selection2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = 'RankStandardisedData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = os.listdir(f'./Data/{choice}')\n",
    "filelist.sort()\n",
    "filelist = filelist[1:]\n",
    "# Remove the first file (an ipynb checkpoint file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we need to perform tests to evaluate this final model, still need to use do this\n",
    "\n",
    "# Gets list of emperical test games (full 2021 season)\n",
    "final_test_games = [file for file in filelist if '2021' in file]\n",
    "\n",
    "# Gathers full games list (except 2021) and performs a single Train-Test Split (note different from previous KFold)\n",
    "test_train_games = [file for file in filelist if '2021' not in file]\n",
    "train_games, test_games = train_test_split(test_train_games, train_size = 0.8, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pre-prepared sample data of trained data only \n",
    "# (the same rows as if we used concatenated all the data from the train_games list)\n",
    "train_data = pd.read_csv('./Models/TrainingData/M3_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kicks BTRS',\n",
       " 'Disposals BTRS',\n",
       " 'Contested Possessions BTRS',\n",
       " 'Effective Disposals BTRS',\n",
       " 'Score Involvements BTRS',\n",
       " 'Metres Gained BTRS',\n",
       " 'Behind Assists BTRS']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Columns of Both Teams Stats only\n",
    "cols = [col for col in train_data.columns if ('BTRS' in col or 'Winloss' in col)]\n",
    "\n",
    "# Select Columns with correlation higher than 0.25 only\n",
    "corr = dict()\n",
    "for col in cols:\n",
    "    corr[col] = train_data[[col, 'Brownlow Votes']].corr(method = 'pearson').loc[col]['Brownlow Votes']\n",
    "\n",
    "corr = list(corr.items())\n",
    "\n",
    "selected_features = [col[0] for col in corr if col[1] > 0.25]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Trains Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*No need to run feature_selection2 because this utilises rule 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains LR model\n",
    "traindata_x_1 = train_data[selected_features]\n",
    "traindata_x_1.index = range(0,len(traindata_x_1))\n",
    "traindata_y_1 = train_data['Brownlow Votes']\n",
    "traindata_y_1.index = range(0,len(traindata_y_1))\n",
    "\n",
    "lm_1 = linear_model.LinearRegression()\n",
    "traindata_x_1 = traindata_x_1.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "model_1 = lm_1.fit(traindata_x_1, traindata_y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and observations\n",
    "predictions_1, testdata_y_1 = predict(test_games, lm_1, selected_features, choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get True Positive/True Negative results\n",
    "result1_1, result2_1 = test(predictions_1, testdata_y_1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9617637661092768,\n",
       "  0.015230588987112579,\n",
       "  0.013526467142400682,\n",
       "  0.009479177761209927],\n",
       " [0.6899563318777293,\n",
       "  0.13100436681222707,\n",
       "  0.10480349344978165,\n",
       "  0.07423580786026202],\n",
       " [0.5545851528384279,\n",
       "  0.12663755458515283,\n",
       "  0.1222707423580786,\n",
       "  0.1965065502183406],\n",
       " [0.3231441048034934,\n",
       "  0.11790393013100436,\n",
       "  0.2183406113537118,\n",
       "  0.3406113537117904]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TP/TN based on what was predicted\n",
    "result1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9617637661092768,\n",
       "  0.01682820321652998,\n",
       "  0.013526467142400682,\n",
       "  0.007881563531792523],\n",
       " [0.6244541484716157,\n",
       "  0.13100436681222707,\n",
       "  0.12663755458515283,\n",
       "  0.11790393013100436],\n",
       " [0.5545851528384279,\n",
       "  0.10480349344978165,\n",
       "  0.1222707423580786,\n",
       "  0.2183406113537118],\n",
       " [0.388646288209607,\n",
       "  0.07423580786026202,\n",
       "  0.1965065502183406,\n",
       "  0.3406113537117904]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TP/TN based on what was observed\n",
    "result2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9617637661092768,\n",
       " 0.13100436681222707,\n",
       " 0.1222707423580786,\n",
       " 0.3406113537117904)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only the True Positive Values\n",
    "return_tp(result1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Summary Observations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Emperical Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the season 2021 data onto predictor and gets top players\n",
    "leaderboard1 = wholeseason(final_test_games, lm_1, selected_features, choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Clayton Oliver', 33),\n",
       " ('Jack Steele', 33),\n",
       " ('Oliver Wines', 32),\n",
       " ('Darcy Parish', 32),\n",
       " ('Jackson Macrae', 31),\n",
       " ('Jarryd Lyons', 30),\n",
       " ('Sam Walsh', 28),\n",
       " ('Christian Petracca', 27),\n",
       " ('Touk Miller', 27),\n",
       " ('Travis Boak', 23),\n",
       " ('Rory Laird', 23),\n",
       " ('Marcus Bontempelli', 23),\n",
       " ('Cameron Guthrie', 21),\n",
       " ('Dayne Zorko', 21),\n",
       " ('Zachary Merrett', 19)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard1[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Predictor's r scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1367427545098272\n"
     ]
    }
   ],
   "source": [
    "print(lm_1.score(traindata_x_1, traindata_y_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Picklising**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Models/M3.pickle', 'wb') as f:\n",
    "    pickle.dump([lm_1, selected_features, choice], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
