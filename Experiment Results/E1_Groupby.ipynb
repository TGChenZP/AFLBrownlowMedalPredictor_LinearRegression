{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Preparation 1. Groupby  #\n",
    "## For Brownlow Predictor Project ##\n",
    "\n",
    "Group different folds of the same model together for further experiment.\n",
    "\n",
    "**Author: `Lang (Ron) Chen` 2021.12-2022.1**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('./Script Output/R.csv')\n",
    "file2 = pd.read_csv('./Script Output/(B).csv')\n",
    "file3 = pd.read_csv('./Script Output/(2)(B).csv')\n",
    "file4 = pd.read_csv('./Script Output/(2).csv')\n",
    "\n",
    "new = pd.concat([file, file2, file3, file4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Initial Manipulation and Defining Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = new[new['Datatype'] != 'PN'] # As PN was proven to be same as N after running scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in the data of the top 6 pollers in 2021\n",
    "PLAYERS = ['Oliver Wines', 'Marcus Bontempelli', 'Clayton Oliver', 'Sam Walsh', 'Darcy Parish', 'Jack Steele']\n",
    "VOTES = [36, 33, 31, 30, 26, 26]\n",
    "PDICT = {i:j for i, j in enumerate(PLAYERS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial datframe\n",
    "data1 = pd.DataFrame({\n",
    "    'Method': list(),\n",
    "    'Datatype': list(),\n",
    "    'Use': list(),\n",
    "    'FS_Val' : list(),\n",
    "    'FS_Rule' : list(),\n",
    "    'WinLoss' : list(),\n",
    "    \n",
    "    'tp3' : list(),\n",
    "    'tp2' : list(),\n",
    "    'tp1' : list(),\n",
    "    'tp0.5' : list(),\n",
    "    'tp0' : list(),\n",
    "    'coef_avg' : list(),\n",
    "    'coef1' : list(),\n",
    "    'coef2_1' : list(),\n",
    "    'coef2_2' : list(),\n",
    "    \n",
    "    'm1_Top3score' : list(),  \n",
    "    'm1_avgvotediff(3)' : list(),\n",
    "    'm1_Top4score' : list(),  \n",
    "    'm1_avgvotediff(4)' : list(),\n",
    "    'm2_Winner' : list(),\n",
    "    'm2_avgvotediff' : list(),\n",
    "    'v1': list(),\n",
    "    'v1m': list()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Defining some functions to find desired scores to rank emperical observations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top3score(model, n):\n",
    "    model.index = range(len(model))\n",
    "    tmp = [findscore(model, i, n) for i in range(n)]\n",
    "    \n",
    "    return sum(tmp)\n",
    "  \n",
    "    \n",
    "    \n",
    "def findscore(model, i, n):\n",
    "    score = list()\n",
    "    \n",
    "    for j in range(len(model)):\n",
    "        diff = -1\n",
    "        for k in range(n):\n",
    "            if PDICT[i] == model.loc[j][f'P{k+1}']:\n",
    "                diff = abs(k-i)\n",
    "                break\n",
    "        \n",
    "        if diff != -1:\n",
    "            score.append((n-i)*(n-diff)/n)\n",
    "        else:\n",
    "            score.append(0)\n",
    "            \n",
    "    return sum(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m1_votediff1(model, n):\n",
    "    i = 0\n",
    "    sumdiff = 0\n",
    "    for i in range(n):\n",
    "        a, b = m1_votediff1_help(model, i)\n",
    "        i += a\n",
    "        sumdiff += b\n",
    "    \n",
    "    if i:\n",
    "        return sumdiff/i\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "def m1_votediff1_help(model, i):\n",
    "    winner = model[model[f'P{i+1}'] == PLAYERS[i]]\n",
    "    winner.index = range(len(winner))\n",
    "    \n",
    "    if len(winner):\n",
    "        diff = [abs(winner.loc[j][f'V{i+1}']- VOTES[i]) for j in range(len(winner))]\n",
    "        return sum(diff), len(winner)\n",
    "    \n",
    "    return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2_win(model):\n",
    "    return len(model[model['P1'] == PLAYERS[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2_votediff(model):\n",
    "    winner = model[model['P1'] == PLAYERS[0]]\n",
    "    winner.index = range(len(winner))\n",
    "    \n",
    "    if len(winner):\n",
    "        diff = [abs(winner.loc[i]['V1']- VOTES[0]) for i in range(len(winner))]\n",
    "        return sum(diff)/len(winner)\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Run Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char, model in file.groupby(['Method', 'Datatype', 'Use', 'Feature Selection Value', 'Feature Selection Rule', 'Winloss']):\n",
    "    \n",
    "    twostep = False\n",
    "    if char[0][0:5] == 'LR(2)':\n",
    "        twostep = True\n",
    "    \n",
    "    hasnan = False\n",
    "    if model['TP0'].hasnans:\n",
    "        hasnan = True\n",
    "    \n",
    "    if not hasnan:\n",
    "        \n",
    "        if not twostep:\n",
    "            tmp = pd.DataFrame({\n",
    "            'Method': char[0],\n",
    "            'Datatype': char[1],\n",
    "            'Use': char[2],\n",
    "            'FS_Val' : char[3],\n",
    "            'FS_Rule' : char[4],\n",
    "            'WinLoss' : char[5],\n",
    "\n",
    "            'tp3' : model['TP3'].mean(),\n",
    "            'tp2' : model['TP2'].mean(),\n",
    "            'tp1' : model['TP1'].mean(),\n",
    "            'tp0.5' : [None],\n",
    "            'tp0' : model['TP0'].mean(),\n",
    "            'coef_avg' : model['Coef1'].mean(),\n",
    "            'coef1' : model['Coef1'].mean(),\n",
    "            'coef2_1' : None,\n",
    "            'coef2_2' : None,\n",
    "            \n",
    "            'm1_Top3score' : top3score(model, 3),\n",
    "            'm1_avgvotediff(3)' : m1_votediff1(model, 3),\n",
    "            'm1_Top4score' : top3score(model, 4),\n",
    "            'm1_avgvotediff(4)' : m1_votediff1(model, 4),\n",
    "            'm2_Winner' : m2_win(model),\n",
    "            'm2_avgvotediff' : m2_votediff(model),\n",
    "            'v1': model['V1'].mean(),\n",
    "            'v1m': model['V1'].median()})\n",
    "            \n",
    "        else:\n",
    "            model.index = range(5)\n",
    "            tmp = pd.DataFrame({\n",
    "            'Method': char[0],\n",
    "            'Datatype': char[1],\n",
    "            'Use': char[2],\n",
    "            'FS_Val' : char[3],\n",
    "            'FS_Rule' : char[4],\n",
    "            'WinLoss' : char[5],\n",
    "\n",
    "            'tp3' : model['TP3'].mean(),\n",
    "            'tp2' : model['TP2'].mean(),\n",
    "            'tp1' : model['TP1'].mean(),\n",
    "            'tp0.5' : model['TP0.5'].mean(),\n",
    "            'tp0' : model['TP0'].mean(),\n",
    "            'coef_avg' : (model['Coef1'].mean() + model['Coef2'].mean())/2,\n",
    "            'coef1' : None,\n",
    "            'coef2_1' : model['Coef1'].mean(),\n",
    "            'coef2_2' : model['Coef2'].mean(),\n",
    "            \n",
    "            'm1_Top3score' : top3score(model, 3),\n",
    "            'm1_avgvotediff(3)' : m1_votediff1(model, 3),\n",
    "            'm1_Top4score' : top3score(model, 4),\n",
    "            'm1_avgvotediff(4)' : m1_votediff1(model, 4),\n",
    "            'm2_Winner' : m2_win(model),\n",
    "            'm2_avgvotediff' : m2_votediff(model),\n",
    "            'v1': [model['V1'].mean()],\n",
    "            'v1m': [model['V1'].median()]})\n",
    "        data1 = pd.concat([data1, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv('All.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
